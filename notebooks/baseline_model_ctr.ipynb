{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "import time\n",
    "import json\n",
    "from collections import deque, defaultdict\n",
    "from codecarbon import EmissionsTracker\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names\n",
    "col_behaviors = ['ImpressionId', 'User', 'Time', 'History', 'Impressions']\n",
    "col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
    "\n",
    "# Read TSV files with Pandas\n",
    "behaviors_train = pd.read_csv(\"data/train/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_train = pd.read_csv(\"data/train/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_val = pd.read_csv(\"data/validation/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_val = pd.read_csv(\"data/validation/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_test = pd.read_csv(\"data/test/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_test = pd.read_csv(\"data/test/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "# zip train and val files\n",
    "behaviors_train_val = pd.concat([behaviors_train, behaviors_val])\n",
    "news_train_val = pd.concat([news_train, news_val])\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_train_val['Timestamp'] = behaviors_train_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_train_val = behaviors_train_val.sort_values(by='Timestamp')\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_val['Timestamp'] = behaviors_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_val = behaviors_val.sort_values(by='Timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Carbon Emissions Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emissions tracker\n",
    "tracker = EmissionsTracker(project_name=\"news_recommendation_ctr_baseline\", output_dir=\"emissions\", log_level=\"critical\")\n",
    "# Start tracking emissions\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction file 'prediction_val_baseline_ctr_1w.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Define rolling window duration: testing with different time windows\n",
    "# TIME_WINDOW = 24 * 60 * 60  # 24 hours in seconds\n",
    "# TIME_WINDOW = 48 * 60 * 60  # 48 hours in seconds\n",
    "# TIME_WINDOW = 72 * 60 * 60  # 72 hours in seconds\n",
    "# TIME_WINDOW = 12 * 60 * 60  # 12 hours in seconds\n",
    "TIME_WINDOW = 7 * 24 * 60 * 60  # 7 days in seconds\n",
    "\n",
    "# Initialize dictionary to store per-news statistics (clicks and impressions with timestamps)\n",
    "news_stats = defaultdict(lambda: {'clicks': deque(), 'impressions': deque()})\n",
    "\n",
    "# Get timestamp of the first impression in the validation set\n",
    "first_impression_time = behaviors_val.iloc[0]['Timestamp']\n",
    "\n",
    "# Filter training/validation data: only keep interactions within the 24h window before validation starts\n",
    "behaviors_train_val = behaviors_train_val[\n",
    "    (behaviors_train_val['Timestamp'] >= first_impression_time - TIME_WINDOW) &\n",
    "    (behaviors_train_val['Timestamp'] < first_impression_time)\n",
    "]\n",
    "\n",
    "# Populate the initial news_stats dictionary with clicks and impressions\n",
    "for _, row in behaviors_train_val.iterrows():\n",
    "    if row['Impressions'] != '-':\n",
    "        for news in row['Impressions'].split():\n",
    "            news_id, label = news.split('-')  # Separate news ID and click label\n",
    "            news_stats[news_id]['impressions'].append(row['Timestamp'])  # Store impression timestamp\n",
    "            if label == '1':  # If clicked, also store in click stats\n",
    "                news_stats[news_id]['clicks'].append(row['Timestamp'])\n",
    "\n",
    "\n",
    "def update_news_stats(current_time, past_clicked_articles, past_impressed_articles):\n",
    "    \"\"\"\n",
    "    Maintain the rolling 24h window:\n",
    "    - Remove outdated clicks/impressions\n",
    "    - Add recent ones from the previous impression\n",
    "    \"\"\"\n",
    "    for news_id in list(news_stats.keys()):\n",
    "        # Remove outdated clicks\n",
    "        while news_stats[news_id]['clicks'] and news_stats[news_id]['clicks'][0] < current_time - TIME_WINDOW:\n",
    "            news_stats[news_id]['clicks'].popleft()\n",
    "        # Remove outdated impressions\n",
    "        while news_stats[news_id]['impressions'] and news_stats[news_id]['impressions'][0] < current_time - TIME_WINDOW:\n",
    "            news_stats[news_id]['impressions'].popleft()\n",
    "        # Remove news entry if both lists are empty\n",
    "        if not news_stats[news_id]['clicks'] and not news_stats[news_id]['impressions']:\n",
    "            del news_stats[news_id]\n",
    "\n",
    "    if past_impressed_articles:\n",
    "        # Unpack impression articles and timestamp\n",
    "        impression_list, timestamp = past_impressed_articles\n",
    "\n",
    "        # Add current impressions\n",
    "        for news_id in impression_list:\n",
    "            news_stats[news_id]['impressions'].append(timestamp)\n",
    "\n",
    "        # Add current clicks\n",
    "        for news_id in past_clicked_articles:\n",
    "            news_stats[news_id]['clicks'].append(timestamp)\n",
    "\n",
    "\n",
    "def rank_news(user_impressions, current_time, past_clicked_articles, past_impressed_articles):\n",
    "    \"\"\"\n",
    "    Rank news articles by Click-Through Rate (CTR) over the past 24 hours.\n",
    "    CTR = clicks / impressions\n",
    "    \"\"\"\n",
    "    # Update rolling stats with recent data\n",
    "    update_news_stats(current_time, past_clicked_articles, past_impressed_articles)\n",
    "\n",
    "    news_rank = []\n",
    "    for news_id in user_impressions:\n",
    "        stats = news_stats.get(news_id, {'clicks': deque(), 'impressions': deque()})\n",
    "        impressions = len(stats['impressions'])\n",
    "        clicks = len(stats['clicks'])\n",
    "        ctr = clicks / impressions if impressions > 0 else 0.0  # Avoid division by zero\n",
    "        news_rank.append((news_id, ctr))\n",
    "\n",
    "    # Sort news descending by CTR\n",
    "    news_rank.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [news_id for news_id, _ in news_rank]\n",
    "\n",
    "\n",
    "def rank_submission_format(user_impressions, current_time, past_clicked_articles, past_impressed_articles):\n",
    "    \"\"\"\n",
    "    Return list of rank positions for each news article in the original impression list.\n",
    "    \"\"\"\n",
    "    ranked_news = rank_news(user_impressions, current_time, past_clicked_articles, past_impressed_articles)\n",
    "    return [ranked_news.index(news_id) + 1 for news_id in user_impressions]\n",
    "\n",
    "\n",
    "def generate_prediction_file(behaviors_df, output_file=\"prediction.txt\"):\n",
    "    \"\"\"\n",
    "    Generate a prediction file with CTR-based news rankings for each impression event.\n",
    "    \"\"\"\n",
    "    past_clicked_articles = []        # List of clicked articles from previous row\n",
    "    past_impressed_articles = []      # Tuple: (list of all impressions, timestamp)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in behaviors_df.iterrows():\n",
    "            impression_id = row['ImpressionId']\n",
    "            current_time = row['Timestamp']\n",
    "            impression_entries = row['Impressions'].split()\n",
    "            user_impressions = [news.split(\"-\")[0] for news in impression_entries]\n",
    "\n",
    "            # Generate rank positions based on CTR\n",
    "            ranked_positions = rank_submission_format(user_impressions, current_time, past_clicked_articles, past_impressed_articles)\n",
    "\n",
    "            # Write prediction result to file in required format\n",
    "            f.write(f\"{impression_id} {json.dumps(ranked_positions)}\\n\")\n",
    "\n",
    "            # Extract clicked and all impression articles from current row\n",
    "            past_clicked_articles = [news.split(\"-\")[0] for news in impression_entries if news.split(\"-\")[1] == '1']\n",
    "            past_impressed_articles = (user_impressions, current_time)\n",
    "\n",
    "    print(f\"‚úÖ Prediction file '{output_file}' successfully created.\")\n",
    "\n",
    "\n",
    "# Run the CTR-based prediction file generation\n",
    "generate_prediction_file(behaviors_val, output_file=\"prediction_val_baseline_ctr_1w.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Emissions Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Carbon emissions from this run: 0.000062 kg CO2eq\n",
      "\n",
      "Detailed emissions data:\n",
      "- Duration: 0.26 hours\n",
      "- Energy consumed: 0.0021 kWh\n",
      "- CPU Power: 5.00 W\n",
      "- GPU Power: 0.00 W\n",
      "- Country: Norway\n"
     ]
    }
   ],
   "source": [
    "# Stop tracking and get the emissions data\n",
    "emissions = tracker.stop()\n",
    "print(f\"üí° Carbon emissions from this run: {emissions:.6f} kg CO2eq\")\n",
    "\n",
    "# Display detailed emissions information and write to txt\n",
    "try:\n",
    "    # Load latest emissions entry\n",
    "    df = pd.read_csv(\"emissions/emissions.csv\")\n",
    "    emissions_data = df.iloc[-1]\n",
    "\n",
    "    # Diagnose available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    # print(f\"üìÇ Available columns: {available_columns}\")\n",
    "\n",
    "    # Prepare values\n",
    "    duration_hr = emissions_data['duration'] / 3600\n",
    "    energy_kwh = emissions_data['energy_consumed']\n",
    "    cpu_power = emissions_data['cpu_power']\n",
    "\n",
    "    gpu_power = (\n",
    "        f\"{emissions_data['gpu_power']:.2f} W\"\n",
    "        if 'gpu_power' in emissions_data and not pd.isna(emissions_data['gpu_power'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    country = emissions_data['country_name'] if 'country_name' in emissions_data else \"Not available\"\n",
    "\n",
    "    carbon_intensity = (\n",
    "        f\"{emissions_data['country_co2_eq_electricity']:.2f} gCO2eq/kWh\"\n",
    "        if 'country_co2_eq_electricity' in emissions_data and not pd.isna(emissions_data['country_co2_eq_electricity'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nDetailed emissions data:\")\n",
    "    print(f\"- Duration: {duration_hr:.2f} hours\")\n",
    "    print(f\"- Energy consumed: {energy_kwh:.4f} kWh\")\n",
    "    print(f\"- CPU Power: {cpu_power:.2f} W\")\n",
    "    print(f\"- GPU Power: {gpu_power}\")\n",
    "    print(f\"- Country: {country}\")\n",
    "\n",
    "    # Create structured report text\n",
    "    report = f\"\"\"\\\n",
    "üìÑ Emissions Report ‚Äì {timestamp}\n",
    "====================================\n",
    "üå± Total Emissions:     {emissions:.6f} kg CO2eq\n",
    "\n",
    "üïí Duration:            {duration_hr:.2f} hours\n",
    "‚ö° Energy Consumed:     {energy_kwh:.4f} kWh\n",
    "üß† CPU Power:           {cpu_power:.2f} W\n",
    "üéÆ GPU Power:           {gpu_power}\n",
    "\n",
    "üåç Country:             {country}\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"emissions\", exist_ok=True)\n",
    "\n",
    "    # Save to .txt file\n",
    "    with open(\"emissions/emissions_report_baseline_ctr_1w.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùó Could not load detailed emissions data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a truth file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Truth file 'truth_val.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth file for validation set\n",
    "def generate_truth_file(validation_impressions, output_file=\"truth.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a truth.txt file with ground truth click labels.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for impression_id, news_list in validation_impressions.items():\n",
    "            labels = [int(news.split(\"-\")[1]) for news in news_list]  # Click labels\n",
    "            f.write(f\"{impression_id} {json.dumps(labels)}\\n\")  # Format output\n",
    "\n",
    "    print(f\"‚úÖ Truth file '{output_file}' successfully created.\")\n",
    "\n",
    "generate_truth_file(behaviors_val.set_index('ImpressionId')['Impressions'].apply(lambda x: x.split()), output_file=\"truth_val.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
