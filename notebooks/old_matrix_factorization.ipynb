{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c8f3ee",
   "metadata": {
    "papermill": {
     "duration": 0.00708,
     "end_time": "2022-11-10T14:52:50.304148",
     "exception": false,
     "start_time": "2022-11-10T14:52:50.297068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building a Collaborative Filtering news recommender\n",
    "In this notebook we preprocess and train a news recommender system based on the MIND dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9b6fb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 4.592409,
     "end_time": "2022-11-10T14:52:54.902479",
     "exception": false,
     "start_time": "2022-11-10T14:52:50.310070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c095a",
   "metadata": {
    "papermill": {
     "duration": 0.005628,
     "end_time": "2022-11-10T14:52:54.914036",
     "exception": false,
     "start_time": "2022-11-10T14:52:54.908408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual pre-processing of data\n",
    "There exist a library to preprocess this data, but I find it quite hard to use it. So instead, after a quick look on the data I found it would not be too hard to do the work myself.\n",
    "For the first read, it is not necessary to understand what is being done here, and I will re-introduce the dataset in the modeling section.\n",
    "\n",
    "### behaviors.tsv\n",
    "\n",
    "#### From documentation:  \n",
    "The behaviors.tsv file contains the impression logs and users' news click histories. It has 5 columns divided by the tab symbol:\n",
    "- Impression ID. The ID of an impression.  \n",
    "- User ID. The anonymous ID of a user.  \n",
    "- Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".  \n",
    "- History. The news click history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.  \n",
    "- Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8216b37",
   "metadata": {
    "papermill": {
     "duration": 2.416559,
     "end_time": "2022-11-10T14:52:57.336274",
     "exception": false,
     "start_time": "2022-11-10T14:52:54.919715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_behaviour = pd.read_csv(\n",
    "    \"/kaggle/input/mind-news-dataset/MINDsmall_train/behaviors.tsv\", \n",
    "    sep=\"\\t\",\n",
    "    names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n",
    "\n",
    "print(f\"The dataset consist of {len(raw_behaviour)} number of interactions.\")\n",
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9f103",
   "metadata": {
    "papermill": {
     "duration": 0.124464,
     "end_time": "2022-11-10T14:52:57.467073",
     "exception": false,
     "start_time": "2022-11-10T14:52:57.342609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Indexize users\n",
    "unique_userIds = raw_behaviour['userId'].unique()\n",
    "# Allocate a unique index for each user, but let the zeroth index be a UNK index:\n",
    "ind2user = {idx +1: itemid for idx, itemid in enumerate(unique_userIds)}\n",
    "user2ind = {itemid : idx for idx, itemid in ind2user.items()}\n",
    "print(f\"We have {len(user2ind)} unique users in the dataset\")\n",
    "\n",
    "# Create a new column with userIdx:\n",
    "raw_behaviour['userIdx'] = raw_behaviour['userId'].map(lambda x: user2ind.get(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cedbe",
   "metadata": {
    "papermill": {
     "duration": 0.021272,
     "end_time": "2022-11-10T14:52:57.494499",
     "exception": false,
     "start_time": "2022-11-10T14:52:57.473227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206dffb1",
   "metadata": {
    "papermill": {
     "duration": 0.00595,
     "end_time": "2022-11-10T14:52:57.506783",
     "exception": false,
     "start_time": "2022-11-10T14:52:57.500833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load article data\n",
    "We also need to get the content information of each article. We will use the news.tsv file to index the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a491a8c",
   "metadata": {
    "papermill": {
     "duration": 0.981184,
     "end_time": "2022-11-10T14:52:58.494028",
     "exception": false,
     "start_time": "2022-11-10T14:52:57.512844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\n",
    "    \"/kaggle/input/mind-news-dataset/MINDsmall_train/news.tsv\", \n",
    "    sep=\"\\t\",\n",
    "    names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "news.head(2)\n",
    "# Build index of items\n",
    "ind2item = {idx +1: itemid for idx, itemid in enumerate(news['itemId'].values)}\n",
    "item2ind = {itemid : idx for idx, itemid in ind2item.items()}\n",
    "\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9347e7",
   "metadata": {
    "papermill": {
     "duration": 0.006171,
     "end_time": "2022-11-10T14:52:58.507059",
     "exception": false,
     "start_time": "2022-11-10T14:52:58.500888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to process the click history and impressions. We need to both indexize the strings, but also to decode impressions into clicks and non-clicks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064c944",
   "metadata": {
    "papermill": {
     "duration": 1.779164,
     "end_time": "2022-11-10T14:53:00.292727",
     "exception": false,
     "start_time": "2022-11-10T14:52:58.513563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indexize click history field\n",
    "def process_click_history(s):\n",
    "    list_of_strings = str(s).split(\" \")\n",
    "    return [item2ind.get(l, 0) for l in list_of_strings]\n",
    "        \n",
    "raw_behaviour['click_history_idx'] = raw_behaviour.click_history.map(lambda s:  process_click_history(s))\n",
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc16d2",
   "metadata": {
    "papermill": {
     "duration": 6.05605,
     "end_time": "2022-11-10T14:53:06.355588",
     "exception": false,
     "start_time": "2022-11-10T14:53:00.299538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect one click and one no-click from impressions:\n",
    "def process_impression(s):\n",
    "    list_of_strings = s.split(\" \")\n",
    "    itemid_rel_tuple = [l.split(\"-\") for l in list_of_strings]\n",
    "    noclicks = []\n",
    "    for entry in itemid_rel_tuple:\n",
    "        if entry[1] =='0':\n",
    "            noclicks.append(entry[0])\n",
    "        if entry[1] =='1':\n",
    "            click = entry[0]\n",
    "    return noclicks, click\n",
    "\n",
    "raw_behaviour['noclicks'], raw_behaviour['click'] = zip(*raw_behaviour['impressions'].map(process_impression))\n",
    "# We can then indexize these two new columns:\n",
    "raw_behaviour['noclicks'] = raw_behaviour['noclicks'].map(lambda list_of_strings: [item2ind.get(l, 0) for l in list_of_strings])\n",
    "raw_behaviour['click'] = raw_behaviour['click'].map(lambda x: item2ind.get(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fced3",
   "metadata": {
    "papermill": {
     "duration": 0.029034,
     "end_time": "2022-11-10T14:53:06.391903",
     "exception": false,
     "start_time": "2022-11-10T14:53:06.362869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ae78a",
   "metadata": {
    "papermill": {
     "duration": 16.534559,
     "end_time": "2022-11-10T14:53:22.933628",
     "exception": false,
     "start_time": "2022-11-10T14:53:06.399069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert timestamp value to hours since epoch\n",
    "raw_behaviour['epochhrs'] = pd.to_datetime(raw_behaviour['timestamp']).values.astype(np.int64)/(1e6)/1000/3600\n",
    "raw_behaviour['epochhrs'] = raw_behaviour['epochhrs'].round()\n",
    "\n",
    "## find first publish date\n",
    "#raw_behaviour[['click','epochhrs']].groupby(\"click\").min(\"epochhrs\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c819b",
   "metadata": {
    "papermill": {
     "duration": 0.00686,
     "end_time": "2022-11-10T14:53:22.947820",
     "exception": false,
     "start_time": "2022-11-10T14:53:22.940960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Output of data preprocessing\n",
    "In this preprocessing we have processed behaviour data, article data and user data.\n",
    "Most importantly, we have indexized users and items in the behaviour dataframe, \n",
    "as pytorch requires integer indicies instead of strings for user and item IDs.\n",
    "\n",
    "- Dataframe`behaviour` contains all interactions: epochhrs (a timestamp), clicks, no clicks and historical clicks per user for each interaction\n",
    "- Dictionary `ind2item`: mapping the item indicies given in behaviour to the real item Id given in the dataset.\n",
    "- Dictionary `ind2user`: mapping the user indicies given in behaviour to the real user Id given in the dataset.\n",
    "- Dataframe `news`: content data on items. We will not use this in the first iteration.\n",
    "\n",
    "The main component is `behaviour`, and for collaborative filtering purposes this is all we need.\n",
    "However, if we want to utilize content data on the news items some preprocessing on the `news` dataframe must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1133a4",
   "metadata": {
    "papermill": {
     "duration": 0.064507,
     "end_time": "2022-11-10T14:53:23.019162",
     "exception": false,
     "start_time": "2022-11-10T14:53:22.954655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Select the columns that we now want to use for further analysis\n",
    "behaviour = raw_behaviour[['epochhrs','userIdx','click_history_idx','noclicks','click']]\n",
    "behaviour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a17974",
   "metadata": {
    "papermill": {
     "duration": 0.006857,
     "end_time": "2022-11-10T14:53:23.033546",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.026689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling\n",
    "We want to make a matrix factorization model where each user $u$ has a d-dimensional parameter vector $z_u$ and each item $i$ has a parameter vector $v_i$. This implies that we do not need the click_history_idx column when we train the model (why?).\n",
    "\n",
    "Second, to simplify the computation of things we assume that the user only considers two items in each interaction: The item the user clicked on, and the first item in the noclicks list (what are we missing by this assumption?).\n",
    "\n",
    "Hence, our dataset consist of a `behaviour` dataframe and a `news` dataframe with content information of the news articles. We will use a language model to interpret our article data, and for simplicity we will only use the title as the text input here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53d075",
   "metadata": {
    "papermill": {
     "duration": 0.107776,
     "end_time": "2022-11-10T14:53:23.148453",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.040677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "behaviour.loc[:,'noclick'] = behaviour['noclicks'].map(lambda x : x[0])\n",
    "behaviour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8f869",
   "metadata": {
    "papermill": {
     "duration": 0.036146,
     "end_time": "2022-11-10T14:53:23.192185",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.156039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let us use the last 10pct of the data as our validation data:\n",
    "test_time_th = behaviour['epochhrs'].quantile(0.9)\n",
    "train = behaviour[behaviour['epochhrs']< test_time_th]\n",
    "valid =  behaviour[behaviour['epochhrs']>= test_time_th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b0ed8",
   "metadata": {
    "papermill": {
     "duration": 0.021463,
     "end_time": "2022-11-10T14:53:23.221199",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.199736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MindDataset(Dataset):\n",
    "    # A fairly simple torch dataset module that can take a pandas dataframe (as above), \n",
    "    # and convert the relevant fields into a dictionary of arrays that can be used in a dataloader\n",
    "    def __init__(self, df):\n",
    "        # Create a dictionary of tensors out of the dataframe\n",
    "        self.data = {\n",
    "            'userIdx' : torch.tensor(df.userIdx.values),\n",
    "            'click' : torch.tensor(df.click.values),\n",
    "            'noclick' : torch.tensor(df.noclick.values)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data['userIdx'])\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516b3da",
   "metadata": {
    "papermill": {
     "duration": 0.043976,
     "end_time": "2022-11-10T14:53:23.272254",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.228278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build datasets and dataloaders of train and validation dataframes:\n",
    "bs = 1024\n",
    "ds_train = MindDataset(df=train)\n",
    "train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True)\n",
    "ds_valid = MindDataset(df=valid)\n",
    "valid_loader = DataLoader(ds_valid, batch_size=bs, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3509984",
   "metadata": {
    "papermill": {
     "duration": 0.006922,
     "end_time": "2022-11-10T14:53:23.286524",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.279602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "#### Framework\n",
    "We will use pytorch-lightning to define and train our model. It is a high-level framework (similar to fastAI) but with a slightly different way of defining things. It is my personal go-to framework and is very flexible. For more information, see https://pytorch-lightning.readthedocs.io/.\n",
    "\n",
    "#### The model\n",
    "We assume that each interaction goes as follow: the user is presented with two items: the click and no-click item. After the user reviewed both items, she will choose the most relevant one. This can be modeled as a categorical distirbution with two options (yes, you could do binomial). There is a loss function in pytorch for this already, called the `F.cross_entropy` that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae880f",
   "metadata": {
    "papermill": {
     "duration": 0.074131,
     "end_time": "2022-11-10T14:53:23.367829",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.293698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a matrix factorization model\n",
    "class NewsMF(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, dim = 10):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "        self.useremb = nn.Embedding(num_embeddings=num_users, embedding_dim=dim)\n",
    "        self.itememb = nn.Embedding(num_embeddings=num_items, embedding_dim=dim)\n",
    "\n",
    "    def step(self, batch, batch_idx, phase=\"train\"):\n",
    "        batch_size = batch['userIdx'].size(0)\n",
    "        score_click = self.forward(batch[\"userIdx\"], batch[\"click\"])\n",
    "        score_noclick = self.forward(batch[\"userIdx\"], batch[\"noclick\"])         \n",
    "        scores_all = torch.concat((score_click, score_noclick), dim=1)\n",
    "        loss = F.cross_entropy(input=scores_all, target=torch.zeros(batch_size, device=scores_all.device).long())\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        uservec =  self.useremb(users)\n",
    "        itemvec = self.itememb(items)\n",
    "        score = (uservec*itemvec).sum(-1).unsqueeze(-1)\n",
    "        return score\n",
    "               \n",
    "        \n",
    "    def predict_single_user(self, user_idx):\n",
    "        items = torch.arange(0, len(ind2item))\n",
    "        user = torch.zeros_like(items) + user_idx\n",
    "        scores = self.forward(user, items)\n",
    "        recommendations = [item.item() for item in torch.topk(scores, 500, dim=0)[1]]\n",
    "        return recommendations\n",
    "    \n",
    "    def predict(self, users):\n",
    "        recommendations = []\n",
    "        for user in users:\n",
    "            recommendation = self.predict_single_user(user)\n",
    "            recommendations.append(recommendation) \n",
    "        return recommendations        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # for now, just do the same computation as during training\n",
    "        return self.step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "mf_model = NewsMF(num_users=len(ind2user)+1, num_items = len(ind2item)+1, dim=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845a59f",
   "metadata": {
    "papermill": {
     "duration": 260.213024,
     "end_time": "2022-11-10T14:57:43.588317",
     "exception": false,
     "start_time": "2022-11-10T14:53:23.375293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=100, accelerator=\"gpu\")\n",
    "trainer.fit(model=mf_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2de6b9",
   "metadata": {
    "papermill": {
     "duration": 0.018279,
     "end_time": "2022-11-10T14:57:43.625934",
     "exception": false,
     "start_time": "2022-11-10T14:57:43.607655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Performance\n",
    "\n",
    "Here we get a batch of test data and calculate some performance metrics. We only have 1 true value (the item that the user actually clicked), so let's define our own version of accuracy@k as follows:\n",
    "\n",
    "- If the item that the user clicked is in the top 10 predictions, return 1,\n",
    "- Otherwise, return 0.\n",
    "\n",
    "The mean of all predictions are a sort of accuracy@k metric. The difficulty here is of course that we only have 1 true value, otherwise we would use something like precision@k or recall@k, but those don't apply here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b04dd",
   "metadata": {
    "papermill": {
     "duration": 3.652625,
     "end_time": "2022-11-10T14:57:47.297098",
     "exception": false,
     "start_time": "2022-11-10T14:57:43.644473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_batch = next(iter(valid_loader))\n",
    "predictions = mf_model.predict(valid_batch[\"userIdx\"])\n",
    "true_values = [item.item() for item in valid_batch[\"click\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7401d9c",
   "metadata": {
    "papermill": {
     "duration": 0.03979,
     "end_time": "2022-11-10T14:57:47.356255",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.316465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def accuracy_at_k(predictions: List[List], true_values: List):\n",
    "    hits = 0\n",
    "    for preds, true in zip(predictions, true_values):\n",
    "        if true in preds:\n",
    "            hits += 1\n",
    "    return hits / len(true_values)\n",
    "\n",
    "accuracy_at_k(predictions, true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393f55b",
   "metadata": {
    "papermill": {
     "duration": 0.018352,
     "end_time": "2022-11-10T14:57:47.393295",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.374943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Playground\n",
    "\n",
    "The cells below are a playground to inspect single recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92836010",
   "metadata": {
    "papermill": {
     "duration": 0.032317,
     "end_time": "2022-11-10T14:57:47.444026",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.411709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_idx = 5\n",
    "items = torch.arange(0, len(ind2item))\n",
    "user = torch.zeros_like(items) + user_idx\n",
    "recommendations = mf_model.predict_single_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921694d",
   "metadata": {
    "papermill": {
     "duration": 0.039506,
     "end_time": "2022-11-10T14:57:47.502715",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.463209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read news article\n",
    "article_id = behaviour[behaviour.userIdx == user_idx][\"click\"].values[0]\n",
    "news[news.itemId == ind2item[article_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfa441",
   "metadata": {
    "papermill": {
     "duration": 0.04489,
     "end_time": "2022-11-10T14:57:47.566215",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.521325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "news[news.itemId.isin([(ind2item[item + 1])  for item in recommendations])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e9f2e",
   "metadata": {
    "papermill": {
     "duration": 0.018897,
     "end_time": "2022-11-10T14:57:47.604387",
     "exception": false,
     "start_time": "2022-11-10T14:57:47.585490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 308.372447,
   "end_time": "2022-11-10T14:57:51.225920",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-10T14:52:42.853473",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
