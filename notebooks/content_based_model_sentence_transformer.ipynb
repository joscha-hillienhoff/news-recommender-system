{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from codecarbon import EmissionsTracker\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names\n",
    "col_behaviors = ['ImpressionId', 'User', 'Time', 'History', 'Impressions']\n",
    "col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
    "\n",
    "# Read TSV files with Pandas\n",
    "behaviors_train = pd.read_csv(\"data/train/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_train = pd.read_csv(\"data/train/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_val = pd.read_csv(\"data/validation/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_val = pd.read_csv(\"data/validation/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_test = pd.read_csv(\"data/test/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_test = pd.read_csv(\"data/test/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "# zip train and val files\n",
    "behaviors_train_val = pd.concat([behaviors_train, behaviors_val])\n",
    "news_train_val = pd.concat([news_train, news_val])\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_train_val['Timestamp'] = behaviors_train_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_train_val = behaviors_train_val.sort_values(by='Timestamp')\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_val['Timestamp'] = behaviors_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_val = behaviors_val.sort_values(by='Timestamp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup Carbon Emissions Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emissions tracker\n",
    "tracker = EmissionsTracker(project_name=\"news_recommendation_ctr_baseline\", output_dir=\"emissions\", log_level=\"critical\")\n",
    "# Start tracking emissions\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Combination\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_news_text(row):\n",
    "    \"\"\" Combine news text fields into a single string for processing.\"\"\"\n",
    "    return f\"{row['Title']} {row['Abstract']} {row['Category']} {row['SubCat']}\"\n",
    "\n",
    "news_val['combined_text'] = news_val.fillna(\"\").apply(combine_news_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load Sentence Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create Embeddings for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1326/1326 [01:42<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for all combined texts\n",
    "news_embeddings = model.encode(\n",
    "    news_val['combined_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  # Important for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Create a similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(news_embeddings, news_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Ranks news articles in an impression based on similarity to the user's clicked news.\n",
    "    \"\"\"\n",
    "    # Create a mapping from news IDs to their indices for quick lookup\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_ids)}\n",
    "\n",
    "    # Create a dictionary mapping user IDs to their history\n",
    "    user_history_map = behaviors_val.set_index(\"User\")[\"History\"].to_dict()\n",
    "\n",
    "    # If the user has no history, return the impression news as is\n",
    "    if user_id not in user_history_map:\n",
    "        return impression_news  # Default: No history\n",
    "\n",
    "    # Retrieve the user's history\n",
    "    history = user_history_map[user_id]\n",
    "\n",
    "    # If the history is invalid (empty, NaN, or not a string), return the impression news as is\n",
    "    if pd.isna(history) or not isinstance(history, str) or not history.strip():\n",
    "        return impression_news  # No valid history\n",
    "\n",
    "    # Get the indices of the news articles the user has clicked on\n",
    "    clicked_indices = [news_id_to_idx[nid] for nid in history.split() if nid in news_id_to_idx]\n",
    "\n",
    "    # If no valid clicked news articles are found, return the impression news as is\n",
    "    if not clicked_indices:\n",
    "        return impression_news  # No valid clicks with embeddings\n",
    "\n",
    "    # Calculate similarity scores for each news article in the impression\n",
    "    scores = []\n",
    "    for news_id in impression_news:\n",
    "        # Get the index of the current news article\n",
    "        news_idx = news_id_to_idx.get(news_id)\n",
    "        if news_idx is None:\n",
    "            # If the news article is not in the mapping, assign a score of 0\n",
    "            scores.append((news_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Compute similarity scores with the user's clicked news articles\n",
    "        similarity_scores = similarity_matrix[news_idx, clicked_indices]\n",
    "        # Calculate the average similarity score\n",
    "        avg_score = np.mean(similarity_scores) if similarity_scores.size > 0 else 0\n",
    "        scores.append((news_id, avg_score))\n",
    "\n",
    "    # Sort the news articles by their similarity scores in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the ranked list of news article IDs\n",
    "    return [news_id for news_id, _ in scores]\n",
    "\n",
    "\n",
    "def rank_submission_format(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Converts the ranked news articles into the required submission format.\n",
    "    \"\"\"\n",
    "    # Rank the news articles for the user\n",
    "    ranked_news = rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix)\n",
    "    # Return the positions of the original impression news in the ranked list\n",
    "    return [ranked_news.index(news_id) + 1 for news_id in impression_news]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_file(similarity_matrix, output_file=\"prediction.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a prediction.txt file with ranked news for each impression.\n",
    "    \"\"\"\n",
    "    # Preprocessing: extract necessary data once\n",
    "    behaviors = behaviors_val.copy()\n",
    "\n",
    "    # Split the \"Impressions\" column into a list of news IDs\n",
    "    behaviors[\"ImpressionList\"] = behaviors[\"Impressions\"].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to the list of news IDs\n",
    "    user_impressions = behaviors.set_index('ImpressionId')['ImpressionList'].to_dict()\n",
    "\n",
    "    # Get the list of all news IDs and create a mapping from news ID to its index\n",
    "    news_ids = news_val[\"NewsId\"].tolist()\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_ids)}\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to user and history information\n",
    "    user_history_map = behaviors.set_index(\"ImpressionId\")[[\"User\", \"History\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    # Open the output file for writing predictions\n",
    "    with open(output_file, \"w\") as f:\n",
    "        # Iterate over each impression and its associated news list\n",
    "        for impression_id, news_list in user_impressions.items():\n",
    "            # Retrieve user information for the current impression\n",
    "            user_info = user_history_map.get(impression_id)\n",
    "            if user_info is None:\n",
    "                continue  # Skip if no user information is available\n",
    "\n",
    "            # Extract user ID and clean the news list (remove any suffix after '-')\n",
    "            user_id = user_info[\"User\"]\n",
    "            cleaned_news_list = [nid.split(\"-\")[0] for nid in news_list]\n",
    "\n",
    "            # Rank the news articles for the user and get their positions\n",
    "            ranked_positions = rank_submission_format(user_id, cleaned_news_list, news_ids, similarity_matrix)\n",
    "\n",
    "            # Write the impression ID and ranked positions to the output file\n",
    "            f.write(f\"{impression_id} {json.dumps(ranked_positions)}\\n\")\n",
    "\n",
    "    # Print a success message after the file is created\n",
    "    print(f\"✅ Prediction file '{output_file}' successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction file 'prediction_val_cbf_st.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "generate_prediction_file(similarity_matrix, output_file=\"prediction_val_cbf_st.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Output carbon emission report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Carbon emissions from this run: 0.000163 kg CO2eq\n",
      "\n",
      "Detailed emissions data:\n",
      "- Duration: 0.68 hours\n",
      "- Energy consumed: 0.0054 kWh\n",
      "- CPU Power: 5.00 W\n",
      "- GPU Power: 0.00 W\n",
      "- Country: Norway\n"
     ]
    }
   ],
   "source": [
    "# Stop tracking and get the emissions data\n",
    "emissions = tracker.stop()\n",
    "print(f\"💡 Carbon emissions from this run: {emissions:.6f} kg CO2eq\")\n",
    "\n",
    "# Display detailed emissions information and write to txt\n",
    "try:\n",
    "    # Load latest emissions entry\n",
    "    df = pd.read_csv(\"emissions/emissions.csv\")\n",
    "    emissions_data = df.iloc[-1]\n",
    "\n",
    "    # Diagnose available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    # print(f\"📂 Available columns: {available_columns}\")\n",
    "\n",
    "    # Prepare values\n",
    "    duration_hr = emissions_data['duration'] / 3600\n",
    "    energy_kwh = emissions_data['energy_consumed']\n",
    "    cpu_power = emissions_data['cpu_power']\n",
    "\n",
    "    gpu_power = (\n",
    "        f\"{emissions_data['gpu_power']:.2f} W\"\n",
    "        if 'gpu_power' in emissions_data and not pd.isna(emissions_data['gpu_power'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    country = emissions_data['country_name'] if 'country_name' in emissions_data else \"Not available\"\n",
    "\n",
    "    carbon_intensity = (\n",
    "        f\"{emissions_data['country_co2_eq_electricity']:.2f} gCO2eq/kWh\"\n",
    "        if 'country_co2_eq_electricity' in emissions_data and not pd.isna(emissions_data['country_co2_eq_electricity'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nDetailed emissions data:\")\n",
    "    print(f\"- Duration: {duration_hr:.2f} hours\")\n",
    "    print(f\"- Energy consumed: {energy_kwh:.4f} kWh\")\n",
    "    print(f\"- CPU Power: {cpu_power:.2f} W\")\n",
    "    print(f\"- GPU Power: {gpu_power}\")\n",
    "    print(f\"- Country: {country}\")\n",
    "\n",
    "    # Create structured report text\n",
    "    report = f\"\"\"\\\n",
    "📄 Emissions Report – {timestamp}\n",
    "====================================\n",
    "🌱 Total Emissions:     {emissions:.6f} kg CO2eq\n",
    "\n",
    "🕒 Duration:            {duration_hr:.2f} hours\n",
    "⚡ Energy Consumed:     {energy_kwh:.4f} kWh\n",
    "🧠 CPU Power:           {cpu_power:.2f} W\n",
    "🎮 GPU Power:           {gpu_power}\n",
    "\n",
    "🌍 Country:             {country}\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"emissions\", exist_ok=True)\n",
    "\n",
    "    # Save to .txt file\n",
    "    with open(\"emissions/emissions_report_cbf_st.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❗ Could not load detailed emissions data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Create a truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Truth file 'truth_val_1000.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth file for validation set\n",
    "def generate_truth_file(impressions, output_file=\"truth.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a truth.txt file with ground truth click labels.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for impression_id, news_list in impressions.items():\n",
    "            labels = [int(news.split(\"-\")[1]) for news in news_list]  # Click labels\n",
    "            f.write(f\"{impression_id} {json.dumps(labels)}\\n\")  # Format output\n",
    "\n",
    "    print(f\"✅ Truth file '{output_file}' successfully created.\")\n",
    "\n",
    "generate_truth_file(behaviors_val.set_index('ImpressionId')['Impressions'].apply(lambda x: x.split()), output_file=\"truth_val_1000.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
