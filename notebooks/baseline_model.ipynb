{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names\n",
    "col_behaviors = ['ImpressionId', 'User', 'Time', 'History', 'Impressions']\n",
    "col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
    "\n",
    "# Read TSV files with Pandas\n",
    "behaviors_train = pd.read_csv(\"data/train/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_train = pd.read_csv(\"data/train/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_val = pd.read_csv(\"data/validation/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_val = pd.read_csv(\"data/validation/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_test = pd.read_csv(\"data/test/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_test = pd.read_csv(\"data/test/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "# zip train and val files\n",
    "behaviors_train_val = pd.concat([behaviors_train, behaviors_val])\n",
    "news_train_val = pd.concat([news_train, news_val])\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_train_val['Timestamp'] = behaviors_train_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_train_val = behaviors_train_val.sort_values(by='Timestamp')\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_val['Timestamp'] = behaviors_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_val = behaviors_val.sort_values(by='Timestamp')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Carbon Emissions Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emissions tracker\n",
    "tracker = EmissionsTracker(project_name=\"news_recommendation_ctr_baseline\", output_dir=\"emissions\", log_level=\"critical\")\n",
    "# Start tracking emissions\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction file 'prediction_val_baseline.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Define rolling window duration: 24 hours in seconds\n",
    "TIME_WINDOW = 86400\n",
    "\n",
    "# Dictionary to store clicked news articles with associated timestamps\n",
    "news_clicks = {}\n",
    "\n",
    "def update_news_clicks(current_time, past_clicked_articles):\n",
    "    \"\"\"\n",
    "    Update the news_clicks dictionary by removing outdated clicks and adding new ones.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove old entries beyond the 24h time window\n",
    "    for news_id in list(news_clicks.keys()):\n",
    "        # Remove oldest click timestamps that fall outside the 24h window\n",
    "        while news_clicks[news_id] and news_clicks[news_id][0] < current_time - TIME_WINDOW:\n",
    "            news_clicks[news_id].popleft()\n",
    "        # Remove the entry if no clicks remain for this news_id\n",
    "        if not news_clicks[news_id]:\n",
    "            del news_clicks[news_id]\n",
    "\n",
    "    # Step 2: Add new clicks from the previous impression\n",
    "    if past_clicked_articles:\n",
    "        for news_id in past_clicked_articles[0]:\n",
    "            if news_id not in news_clicks:\n",
    "                news_clicks[news_id] = deque()\n",
    "            news_clicks[news_id].append(past_clicked_articles[1])  # Append the timestamp of the click\n",
    "\n",
    "\n",
    "def rank_news(user_impressions, current_time, past_clicked_articles):\n",
    "    \"\"\"\n",
    "    Rank news articles in the current impression based on the number of clicks \n",
    "    in the last 24 hours.\n",
    "    \"\"\"\n",
    "    # Update the global click stats with the current timestamp and past clicks\n",
    "    update_news_clicks(current_time, past_clicked_articles)\n",
    "\n",
    "    news_rank = []\n",
    "    for news_id in user_impressions:\n",
    "        if news_id in news_clicks:\n",
    "            # Use the number of clicks in the past 24h\n",
    "            news_rank.append((news_id, len(news_clicks[news_id])))\n",
    "        else:\n",
    "            # If no clicks, assign score of 0\n",
    "            news_rank.append((news_id, 0))\n",
    "\n",
    "    # Sort the news items by click count in descending order\n",
    "    news_rank.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return only the ordered list of news IDs\n",
    "    return [news_id for news_id, _ in news_rank]\n",
    "\n",
    "\n",
    "def rank_submission_format(user_impressions, current_time, past_clicked_articles):\n",
    "    \"\"\"\n",
    "    Return the ranking positions for each news article in the user impression list.\n",
    "    \"\"\"\n",
    "    ranked_news = rank_news(user_impressions, current_time, past_clicked_articles)\n",
    "    return [ranked_news.index(news_id) + 1 for news_id in user_impressions]\n",
    "\n",
    "\n",
    "def generate_prediction_file(behaviors_df, output_file=\"prediction.txt\"):\n",
    "    \"\"\"\n",
    "    Generate a prediction file with click-based news rankings for each impression.\n",
    "    \"\"\"\n",
    "    past_clicked_articles = []  # Stores clicked articles from the previous row\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in behaviors_df.iterrows():\n",
    "            impression_id = row['ImpressionId']\n",
    "            current_time = row['Timestamp']\n",
    "\n",
    "            # Extract only news IDs (without click label)\n",
    "            user_impressions = [news.split(\"-\")[0] for news in row['Impressions'].split()]\n",
    "\n",
    "            # Compute ranking positions based on past 24h click data\n",
    "            ranked_positions = rank_submission_format(user_impressions, current_time, past_clicked_articles)\n",
    "\n",
    "            # Write results to file in required format\n",
    "            f.write(f\"{impression_id} {json.dumps(ranked_positions)}\\n\")\n",
    "\n",
    "            # Prepare click data from this row for use in the next iteration\n",
    "            past_clicked_articles = (\n",
    "                [news.split(\"-\")[0] for news in row['Impressions'].split() if news.split(\"-\")[1] == '1'],\n",
    "                current_time\n",
    "            )\n",
    "\n",
    "    print(f\"✅ Prediction file '{output_file}' successfully created.\")\n",
    "\n",
    "\n",
    "# Generate predictions for the validation set using the click-based popularity model\n",
    "generate_prediction_file(behaviors_val, output_file=\"prediction_val_baseline.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon Emissions Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Carbon emissions from this run: 0.000001 kg CO2eq\n",
      "\n",
      "Detailed emissions data:\n",
      "- Duration: 0.01 hours\n",
      "- Energy consumed: 0.0000 kWh\n",
      "- CPU Power: 5.00 W\n",
      "- GPU Power: 0.00 W\n",
      "- Country: Norway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joscha/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
     ]
    }
   ],
   "source": [
    "# Stop tracking and get the emissions data\n",
    "emissions = tracker.stop()\n",
    "print(f\"💡 Carbon emissions from this run: {emissions:.6f} kg CO2eq\")\n",
    "\n",
    "# Display detailed emissions information and write to txt\n",
    "try:\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Load latest emissions entry\n",
    "    df = pd.read_csv(\"emissions/emissions.csv\")\n",
    "    emissions_data = df.iloc[-1]\n",
    "\n",
    "    # Diagnose available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    # print(f\"📂 Available columns: {available_columns}\")\n",
    "\n",
    "    # Prepare values\n",
    "    duration_hr = emissions_data['duration'] / 3600\n",
    "    energy_kwh = emissions_data['energy_consumed']\n",
    "    cpu_power = emissions_data['cpu_power']\n",
    "\n",
    "    gpu_power = (\n",
    "        f\"{emissions_data['gpu_power']:.2f} W\"\n",
    "        if 'gpu_power' in emissions_data and not pd.isna(emissions_data['gpu_power'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    country = emissions_data['country_name'] if 'country_name' in emissions_data else \"Not available\"\n",
    "\n",
    "    carbon_intensity = (\n",
    "        f\"{emissions_data['country_co2_eq_electricity']:.2f} gCO2eq/kWh\"\n",
    "        if 'country_co2_eq_electricity' in emissions_data and not pd.isna(emissions_data['country_co2_eq_electricity'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nDetailed emissions data:\")\n",
    "    print(f\"- Duration: {duration_hr:.2f} hours\")\n",
    "    print(f\"- Energy consumed: {energy_kwh:.4f} kWh\")\n",
    "    print(f\"- CPU Power: {cpu_power:.2f} W\")\n",
    "    print(f\"- GPU Power: {gpu_power}\")\n",
    "    print(f\"- Country: {country}\")\n",
    "\n",
    "    # Create structured report text\n",
    "    report = f\"\"\"\\\n",
    "📄 Emissions Report – {timestamp}\n",
    "====================================\n",
    "🌱 Total Emissions:     {emissions:.6f} kg CO2eq\n",
    "\n",
    "🕒 Duration:            {duration_hr:.2f} hours\n",
    "⚡ Energy Consumed:     {energy_kwh:.4f} kWh\n",
    "🧠 CPU Power:           {cpu_power:.2f} W\n",
    "🎮 GPU Power:           {gpu_power}\n",
    "\n",
    "🌍 Country:             {country}\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"emissions\", exist_ok=True)\n",
    "\n",
    "    # Save to .txt file\n",
    "    with open(\"emissions/emissions_report_baseline.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❗ Could not load detailed emissions data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a truth file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Truth file 'truth_val.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth file for validation set\n",
    "def generate_truth_file(validation_impressions, output_file=\"truth.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a truth.txt file with ground truth click labels.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for impression_id, news_list in validation_impressions.items():\n",
    "            labels = [int(news.split(\"-\")[1]) for news in news_list]  # Click labels\n",
    "            f.write(f\"{impression_id} {json.dumps(labels)}\\n\")  # Format output\n",
    "\n",
    "    print(f\"✅ Truth file '{output_file}' successfully created.\")\n",
    "\n",
    "generate_truth_file(behaviors_val.set_index('ImpressionId')['Impressions'].apply(lambda x: x.split()), output_file=\"truth_val.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
