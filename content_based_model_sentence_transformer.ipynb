{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joscha/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from codecarbon import EmissionsTracker\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names\n",
    "col_behaviors = ['ImpressionId', 'User', 'Time', 'History', 'Impressions']\n",
    "col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
    "\n",
    "# Read TSV files with Pandas\n",
    "behaviors_train = pd.read_csv(\"data/train/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_train = pd.read_csv(\"data/train/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_val = pd.read_csv(\"data/validation/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_val = pd.read_csv(\"data/validation/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_test = pd.read_csv(\"data/test/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_test = pd.read_csv(\"data/test/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "# zip train and val files\n",
    "behaviors_train_val = pd.concat([behaviors_train, behaviors_val])\n",
    "news_train_val = pd.concat([news_train, news_val])\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_train_val['Timestamp'] = behaviors_train_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_train_val = behaviors_train_val.sort_values(by='Timestamp')\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_val['Timestamp'] = behaviors_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_val = behaviors_val.sort_values(by='Timestamp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup Carbon Emissions Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emissions tracker\n",
    "tracker = EmissionsTracker(project_name=\"news_recommendation_ctr_baseline\", output_dir=\"emissions\", log_level=\"critical\")\n",
    "# Start tracking emissions\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Combination\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_news_text(row):\n",
    "    \"\"\" Combine news text fields into a single string for processing.\"\"\"\n",
    "    return f\"{row['Title']} {row['Abstract']} {row['Category']} {row['SubCat']}\"\n",
    "\n",
    "news_val['combined_text'] = news_val.fillna(\"\").apply(combine_news_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load Sentence Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create Embeddings for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1326/1326 [01:36<00:00, 13.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for all combined texts\n",
    "news_embeddings = model.encode(\n",
    "    news_val['combined_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  # Important for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Create a similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(news_embeddings, news_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Ranks news articles in an impression based on similarity to the user's clicked news.\n",
    "    \"\"\"\n",
    "    # Create a mapping from news IDs to their indices for quick lookup\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_ids)}\n",
    "\n",
    "    # Create a dictionary mapping user IDs to their history\n",
    "    user_history_map = behaviors_val.set_index(\"User\")[\"History\"].to_dict()\n",
    "\n",
    "    # If the user has no history, return the impression news as is\n",
    "    if user_id not in user_history_map:\n",
    "        return impression_news  # Default: No history\n",
    "\n",
    "    # Retrieve the user's history\n",
    "    history = user_history_map[user_id]\n",
    "\n",
    "    # If the history is invalid (empty, NaN, or not a string), return the impression news as is\n",
    "    if pd.isna(history) or not isinstance(history, str) or not history.strip():\n",
    "        return impression_news  # No valid history\n",
    "\n",
    "    # Get the indices of the news articles the user has clicked on\n",
    "    clicked_indices = [news_id_to_idx[nid] for nid in history.split() if nid in news_id_to_idx]\n",
    "\n",
    "    # If no valid clicked news articles are found, return the impression news as is\n",
    "    if not clicked_indices:\n",
    "        return impression_news  # No valid clicks with embeddings\n",
    "\n",
    "    # Calculate similarity scores for each news article in the impression\n",
    "    scores = []\n",
    "    for news_id in impression_news:\n",
    "        # Get the index of the current news article\n",
    "        news_idx = news_id_to_idx.get(news_id)\n",
    "        if news_idx is None:\n",
    "            # If the news article is not in the mapping, assign a score of 0\n",
    "            scores.append((news_id, 0))\n",
    "            continue\n",
    "\n",
    "        # Compute similarity scores with the user's clicked news articles\n",
    "        similarity_scores = similarity_matrix[news_idx, clicked_indices]\n",
    "        # Calculate the average similarity score\n",
    "        avg_score = np.mean(similarity_scores) if similarity_scores.size > 0 else 0\n",
    "        scores.append((news_id, avg_score))\n",
    "\n",
    "    # Sort the news articles by their similarity scores in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the ranked list of news article IDs\n",
    "    return [news_id for news_id, _ in scores]\n",
    "\n",
    "\n",
    "def rank_submission_format(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Converts the ranked news articles into the required submission format.\n",
    "    \"\"\"\n",
    "    # Rank the news articles for the user\n",
    "    ranked_news = rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix)\n",
    "    # Return the positions of the original impression news in the ranked list\n",
    "    return [ranked_news.index(news_id) + 1 for news_id in impression_news]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_file(similarity_matrix, output_file=\"prediction.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a prediction.txt file with ranked news for each impression.\n",
    "    \"\"\"\n",
    "    # Preprocessing: extract necessary data once\n",
    "    behaviors = behaviors_val.copy()\n",
    "\n",
    "    # Split the \"Impressions\" column into a list of news IDs\n",
    "    behaviors[\"ImpressionList\"] = behaviors[\"Impressions\"].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to the list of news IDs\n",
    "    user_impressions = behaviors.set_index('ImpressionId')['ImpressionList'].to_dict()\n",
    "\n",
    "    # Get the list of all news IDs and create a mapping from news ID to its index\n",
    "    news_ids = news_val[\"NewsId\"].tolist()\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_ids)}\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to user and history information\n",
    "    user_history_map = behaviors.set_index(\"ImpressionId\")[[\"User\", \"History\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    # Open the output file for writing predictions\n",
    "    with open(output_file, \"w\") as f:\n",
    "        # Iterate over each impression and its associated news list\n",
    "        for impression_id, news_list in user_impressions.items():\n",
    "            # Retrieve user information for the current impression\n",
    "            user_info = user_history_map.get(impression_id)\n",
    "            if user_info is None:\n",
    "                continue  # Skip if no user information is available\n",
    "\n",
    "            # Extract user ID and clean the news list (remove any suffix after '-')\n",
    "            user_id = user_info[\"User\"]\n",
    "            cleaned_news_list = [nid.split(\"-\")[0] for nid in news_list]\n",
    "\n",
    "            # Rank the news articles for the user and get their positions\n",
    "            ranked_positions = rank_submission_format(user_id, cleaned_news_list, news_ids, similarity_matrix)\n",
    "\n",
    "            # Write the impression ID and ranked positions to the output file\n",
    "            f.write(f\"{impression_id} {json.dumps(ranked_positions)}\\n\")\n",
    "\n",
    "    # Print a success message after the file is created\n",
    "    print(f\"‚úÖ Prediction file '{output_file}' successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_prediction_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction_val_sbert_optimized_1000.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mgenerate_prediction_file\u001b[0;34m(similarity_matrix, output_file)\u001b[0m\n\u001b[1;32m     32\u001b[0m cleaned_news_list \u001b[38;5;241m=\u001b[39m [nid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m news_list]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Rank the news articles for the user and get their positions\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m ranked_positions \u001b[38;5;241m=\u001b[39m \u001b[43mrank_submission_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaned_news_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Write the impression ID and ranked positions to the output file\u001b[39;00m\n\u001b[1;32m     38\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimpression_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(ranked_positions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mrank_submission_format\u001b[0;34m(user_id, impression_news, news_ids, similarity_matrix)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mConverts the ranked news articles into the required submission format.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Rank the news articles for the user\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m ranked_news \u001b[38;5;241m=\u001b[39m \u001b[43mrank_news_for_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpression_news\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Return the positions of the original impression news in the ranked list\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [ranked_news\u001b[38;5;241m.\u001b[39mindex(news_id) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m news_id \u001b[38;5;129;01min\u001b[39;00m impression_news]\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mrank_news_for_user\u001b[0;34m(user_id, impression_news, news_ids, similarity_matrix)\u001b[0m\n\u001b[1;32m      6\u001b[0m news_id_to_idx \u001b[38;5;241m=\u001b[39m {nid: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, nid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(news_ids)}\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a dictionary mapping user IDs to their history\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m user_history_map \u001b[38;5;241m=\u001b[39m \u001b[43mbehaviors_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# If the user has no history, return the impression news as is\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m user_history_map:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/pandas/core/series.py:2077\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   2074\u001b[0m into_c \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mstandardize_mapping(into)\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n\u001b[0;32m-> 2077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c((k, maybe_box_native(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2079\u001b[0m     \u001b[38;5;66;03m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m     \u001b[38;5;66;03m# indexer return native python type\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/pandas/core/series.py:2077\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2074\u001b[0m into_c \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mstandardize_mapping(into)\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n\u001b[0;32m-> 2077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c((k, \u001b[43mmaybe_box_native\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2079\u001b[0m     \u001b[38;5;66;03m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m     \u001b[38;5;66;03m# indexer return native python type\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m into_c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/1 Projects/Uni/Recommender Systems/TDT4215 Group Project/TDT4215-Group-Project/.venv/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:182\u001b[0m, in \u001b[0;36mmaybe_box_native\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    177\u001b[0m         value \u001b[38;5;241m=\u001b[39m Timedelta(value)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_box_native\u001b[39m(value: Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NAType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NAType:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    If passed a scalar cast the scalar to a python native type.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    scalar or Series\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float(value):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_prediction_file(similarity_matrix, output_file=\"prediction_val_sbert_optimized_1000.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Output carbon emission report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop tracking and get the emissions data\n",
    "emissions = tracker.stop()\n",
    "print(f\"üí° Carbon emissions from this run: {emissions:.6f} kg CO2eq\")\n",
    "\n",
    "# Display detailed emissions information and write to txt\n",
    "try:\n",
    "    # Load latest emissions entry\n",
    "    df = pd.read_csv(\"emissions/emissions.csv\")\n",
    "    emissions_data = df.iloc[-1]\n",
    "\n",
    "    # Diagnose available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    # print(f\"üìÇ Available columns: {available_columns}\")\n",
    "\n",
    "    # Prepare values\n",
    "    duration_hr = emissions_data['duration'] / 3600\n",
    "    energy_kwh = emissions_data['energy_consumed']\n",
    "    cpu_power = emissions_data['cpu_power']\n",
    "\n",
    "    gpu_power = (\n",
    "        f\"{emissions_data['gpu_power']:.2f} W\"\n",
    "        if 'gpu_power' in emissions_data and not pd.isna(emissions_data['gpu_power'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    country = emissions_data['country_name'] if 'country_name' in emissions_data else \"Not available\"\n",
    "\n",
    "    carbon_intensity = (\n",
    "        f\"{emissions_data['country_co2_eq_electricity']:.2f} gCO2eq/kWh\"\n",
    "        if 'country_co2_eq_electricity' in emissions_data and not pd.isna(emissions_data['country_co2_eq_electricity'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nDetailed emissions data:\")\n",
    "    print(f\"- Duration: {duration_hr:.2f} hours\")\n",
    "    print(f\"- Energy consumed: {energy_kwh:.4f} kWh\")\n",
    "    print(f\"- CPU Power: {cpu_power:.2f} W\")\n",
    "    print(f\"- GPU Power: {gpu_power}\")\n",
    "    print(f\"- Country: {country}\")\n",
    "\n",
    "    # Create structured report text\n",
    "    report = f\"\"\"\\\n",
    "üìÑ Emissions Report ‚Äì {timestamp}\n",
    "====================================\n",
    "üå± Total Emissions:     {emissions:.6f} kg CO2eq\n",
    "\n",
    "üïí Duration:            {duration_hr:.2f} hours\n",
    "‚ö° Energy Consumed:     {energy_kwh:.4f} kWh\n",
    "üß† CPU Power:           {cpu_power:.2f} W\n",
    "üéÆ GPU Power:           {gpu_power}\n",
    "\n",
    "üåç Country:             {country}\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"emissions\", exist_ok=True)\n",
    "\n",
    "    # Save to .txt file\n",
    "    with open(\"emissions/emissions_report_content_st.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùó Could not load detailed emissions data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Create a truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Truth file 'truth_val_1000.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth file for validation set\n",
    "def generate_truth_file(impressions, output_file=\"truth.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a truth.txt file with ground truth click labels.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for impression_id, news_list in impressions.items():\n",
    "            labels = [int(news.split(\"-\")[1]) for news in news_list]  # Click labels\n",
    "            f.write(f\"{impression_id} {json.dumps(labels)}\\n\")  # Format output\n",
    "\n",
    "    print(f\"‚úÖ Truth file '{output_file}' successfully created.\")\n",
    "\n",
    "generate_truth_file(behaviors_val.set_index('ImpressionId')['Impressions'].apply(lambda x: x.split()), output_file=\"truth_val_1000.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
