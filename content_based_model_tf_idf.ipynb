{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from codecarbon import EmissionsTracker\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('maxent_ne_chunker_tab')\n",
    "# nltk.download('words')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column names\n",
    "col_behaviors = ['ImpressionId', 'User', 'Time', 'History', 'Impressions']\n",
    "col_news = ['NewsId', 'Category', 'SubCat', 'Title', 'Abstract', 'url', 'TitleEnt', 'AbstractEnt']\n",
    "\n",
    "# Read TSV files with Pandas\n",
    "behaviors_train = pd.read_csv(\"data/train/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_train = pd.read_csv(\"data/train/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_val = pd.read_csv(\"data/validation/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_val = pd.read_csv(\"data/validation/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "behaviors_test = pd.read_csv(\"data/test/behaviors.tsv\", sep=\"\\t\", header=None, names=col_behaviors)\n",
    "news_test = pd.read_csv(\"data/test/news.tsv\", sep=\"\\t\", header=None, names=col_news)\n",
    "\n",
    "# zip train and val files\n",
    "behaviors_train_val = pd.concat([behaviors_train, behaviors_val])\n",
    "news_train_val = pd.concat([news_train, news_val])\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_train_val['Timestamp'] = behaviors_train_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_train_val = behaviors_train_val.sort_values(by='Timestamp')\n",
    "\n",
    "# Convert time column to timestamp and sort by time\n",
    "behaviors_val['Timestamp'] = behaviors_val['Time'].apply(lambda x: time.mktime(time.strptime(x, \"%m/%d/%Y %I:%M:%S %p\")))\n",
    "behaviors_val = behaviors_val.sort_values(by='Timestamp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup Carbon Emissions Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emissions tracker\n",
    "tracker = EmissionsTracker(project_name=\"news_recommendation_ctr_baseline\", output_dir=\"emissions\", log_level=\"critical\")\n",
    "# Start tracking emissions\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Check for missing values and fill them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsId            0\n",
      "Category          0\n",
      "SubCat            0\n",
      "Title             0\n",
      "Abstract       2021\n",
      "url               0\n",
      "TitleEnt          2\n",
      "AbstractEnt       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(news_val.isna().sum())\n",
    "\n",
    "# Fill missing values with empty string\n",
    "news_val = news_val.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Create a combined feature of title abstract and categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title, abstract and categories\n",
    "news_val[\"Combined\"] = news_val[\"Title\"] + \" \" + news_val[\"Abstract\"] + \" \" + news_val[\"Category\"] + \" \" + news_val[\"SubCat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Tokenization, stopword and punctuation removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into lowercase words\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    # Remove punctuation from the tokens\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]  \n",
    "    # Remove stopwords from the tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  \n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing function to the 'Combined' column and create a new column 'ProcessedCombined'\n",
    "news_val['ProcessedCombined'] = news_val['Combined'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42416, 4724)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer with specific parameters\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.001, max_df=0.999, ngram_range=(1, 3))\n",
    "\n",
    "# Convert the tokenized and processed text back into a single string for each row\n",
    "news_val['ProcessedCombined'] = news_val['ProcessedCombined'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Fit the TF-IDF Vectorizer to the processed text and transform it into a sparse matrix of TF-IDF features\n",
    "text_vectors = tfidf.fit_transform(news_val['ProcessedCombined'])\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrix (number of documents, number of features)\n",
    "print(text_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Compute similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(text_vectors, text_vectors)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Rank news articles based on similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Ranks news articles in an impression based on similarity to the user's previous clicked news.\n",
    "    \"\"\"\n",
    "    # Check if the user exists in the validation dataset\n",
    "    if user_id not in behaviors_val[\"User\"].values:\n",
    "        return impression_news  # Default: No history, return as is\n",
    "\n",
    "    # Get the user's previously clicked news from their history\n",
    "    history = behaviors_val[behaviors_val[\"User\"] == user_id][\"History\"].values\n",
    "\n",
    "    # If no history or invalid history type, return the impression news as is\n",
    "    if len(history) == 0 or pd.isna(history[0]) or not isinstance(history[0], str):\n",
    "        return impression_news\n",
    "\n",
    "    # Split the history into individual news IDs\n",
    "    clicked_news = history[0].split()\n",
    "    # Get indices of clicked news in the news_ids list\n",
    "    clicked_indices = [news_ids.index(nid) for nid in clicked_news if nid in news_ids]\n",
    "\n",
    "    # Compute similarity scores for each news article in the impression\n",
    "    scores = []\n",
    "    for news_id in impression_news:\n",
    "        if news_id not in news_ids:\n",
    "            scores.append((news_id, 0))  # Default score if news is missing\n",
    "            continue\n",
    "        \n",
    "        # Get the index of the current news article\n",
    "        news_idx = news_ids.index(news_id)\n",
    "        # Compute similarity scores between the current news and clicked news\n",
    "        similarity_scores = similarity_matrix[news_idx, clicked_indices]\n",
    "        # Calculate the average similarity score\n",
    "        avg_score = np.mean(similarity_scores) if len(similarity_scores) > 0 else 0\n",
    "        scores.append((news_id, avg_score))\n",
    "\n",
    "    # Sort the news articles by their similarity scores in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the ranked list of news IDs\n",
    "    return [news_id for news_id, _ in scores]\n",
    "\n",
    "\n",
    "def rank_submission_format(user_id, impression_news, news_ids, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Formats the ranked news articles for submission.\n",
    "    \"\"\"\n",
    "    # Rank the news articles for the user\n",
    "    ranked_news = rank_news_for_user(user_id, impression_news, news_ids, similarity_matrix)\n",
    "    submission = []\n",
    "    # Create a list of ranks for each news article in the original impression order\n",
    "    for news_id in impression_news:\n",
    "        submission.append(ranked_news.index(news_id) + 1)  # Rank starts from 1\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Create submision file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_file(similarity_matrix, output_file=\"prediction.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a prediction.txt file with ranked news for each impression.\n",
    "    \"\"\"\n",
    "    # Preprocessing: extract necessary data once\n",
    "    behaviors = behaviors_val.copy()\n",
    "\n",
    "    # Split the \"Impressions\" column into a list of news IDs\n",
    "    behaviors[\"ImpressionList\"] = behaviors[\"Impressions\"].apply(lambda x: x.split())\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to the list of news IDs\n",
    "    user_impressions = behaviors.set_index('ImpressionId')['ImpressionList'].to_dict()\n",
    "\n",
    "    # Get the list of all news IDs and create a mapping from news ID to its index\n",
    "    news_ids = news_val[\"NewsId\"].tolist()\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_ids)}\n",
    "\n",
    "    # Create a dictionary mapping ImpressionId to user and history information\n",
    "    user_history_map = behaviors.set_index(\"ImpressionId\")[[\"User\", \"History\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    # Open the output file for writing predictions\n",
    "    with open(output_file, \"w\") as f:\n",
    "        # Iterate over each impression and its associated news list\n",
    "        for impression_id, news_list in user_impressions.items():\n",
    "            # Retrieve user information for the current impression\n",
    "            user_info = user_history_map.get(impression_id)\n",
    "            if user_info is None:\n",
    "                continue  # Skip if no user information is available\n",
    "\n",
    "            # Extract user ID and clean the news list (remove any suffix after '-')\n",
    "            user_id = user_info[\"User\"]\n",
    "            cleaned_news_list = [nid.split(\"-\")[0] for nid in news_list]\n",
    "\n",
    "            # Rank the news articles for the user and get their positions\n",
    "            ranked_positions = rank_submission_format(user_id, cleaned_news_list, news_ids, similarity_matrix)\n",
    "\n",
    "            # Write the impression ID and ranked positions to the output file\n",
    "            f.write(f\"{impression_id} {json.dumps(ranked_positions)}\\n\")\n",
    "\n",
    "    # Print a success message after the file is created\n",
    "    print(f\"‚úÖ Prediction file '{output_file}' successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction file 'prediction_val_tf_idf.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "generate_prediction_file(similarity_matrix, output_file=\"prediction_val_tf_idf.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Output carbon emission report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Carbon emissions from this run: 0.000185 kg CO2eq\n",
      "\n",
      "Detailed emissions data:\n",
      "- Duration: 0.77 hours\n",
      "- Energy consumed: 0.0061 kWh\n",
      "- CPU Power: 5.00 W\n",
      "- GPU Power: 0.00 W\n",
      "- Country: Norway\n"
     ]
    }
   ],
   "source": [
    "# Stop tracking and get the emissions data\n",
    "emissions = tracker.stop()\n",
    "print(f\"üí° Carbon emissions from this run: {emissions:.6f} kg CO2eq\")\n",
    "\n",
    "# Display detailed emissions information and write to txt\n",
    "try:\n",
    "    # Load latest emissions entry\n",
    "    df = pd.read_csv(\"emissions/emissions.csv\")\n",
    "    emissions_data = df.iloc[-1]\n",
    "\n",
    "    # Diagnose available columns\n",
    "    available_columns = df.columns.tolist()\n",
    "    # print(f\"üìÇ Available columns: {available_columns}\")\n",
    "\n",
    "    # Prepare values\n",
    "    duration_hr = emissions_data['duration'] / 3600\n",
    "    energy_kwh = emissions_data['energy_consumed']\n",
    "    cpu_power = emissions_data['cpu_power']\n",
    "\n",
    "    gpu_power = (\n",
    "        f\"{emissions_data['gpu_power']:.2f} W\"\n",
    "        if 'gpu_power' in emissions_data and not pd.isna(emissions_data['gpu_power'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    country = emissions_data['country_name'] if 'country_name' in emissions_data else \"Not available\"\n",
    "\n",
    "    carbon_intensity = (\n",
    "        f\"{emissions_data['country_co2_eq_electricity']:.2f} gCO2eq/kWh\"\n",
    "        if 'country_co2_eq_electricity' in emissions_data and not pd.isna(emissions_data['country_co2_eq_electricity'])\n",
    "        else \"Not available\"\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Print to console\n",
    "    print(f\"\\nDetailed emissions data:\")\n",
    "    print(f\"- Duration: {duration_hr:.2f} hours\")\n",
    "    print(f\"- Energy consumed: {energy_kwh:.4f} kWh\")\n",
    "    print(f\"- CPU Power: {cpu_power:.2f} W\")\n",
    "    print(f\"- GPU Power: {gpu_power}\")\n",
    "    print(f\"- Country: {country}\")\n",
    "\n",
    "    # Create structured report text\n",
    "    report = f\"\"\"\\\n",
    "üìÑ Emissions Report ‚Äì {timestamp}\n",
    "====================================\n",
    "üå± Total Emissions:     {emissions:.6f} kg CO2eq\n",
    "\n",
    "üïí Duration:            {duration_hr:.2f} hours\n",
    "‚ö° Energy Consumed:     {energy_kwh:.4f} kWh\n",
    "üß† CPU Power:           {cpu_power:.2f} W\n",
    "üéÆ GPU Power:           {gpu_power}\n",
    "\n",
    "üåç Country:             {country}\n",
    "====================================\n",
    "\"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(\"emissions\", exist_ok=True)\n",
    "\n",
    "    # Save to .txt file\n",
    "    with open(\"emissions/emissions_report_content_tf_idf.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùó Could not load detailed emissions data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Create a truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Truth file 'truth_val_1000.txt' successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Generate ground truth file for validation set\n",
    "def generate_truth_file(impressions, output_file=\"truth.txt\"):\n",
    "    \"\"\"\n",
    "    Generates a truth.txt file with ground truth click labels.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for impression_id, news_list in impressions.items():\n",
    "            labels = [int(news.split(\"-\")[1]) for news in news_list]  # Click labels\n",
    "            f.write(f\"{impression_id} {json.dumps(labels)}\\n\")  # Format output\n",
    "\n",
    "    print(f\"‚úÖ Truth file '{output_file}' successfully created.\")\n",
    "\n",
    "generate_truth_file(behaviors_val.set_index('ImpressionId')['Impressions'].apply(lambda x: x.split()), output_file=\"truth_val_1000.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
